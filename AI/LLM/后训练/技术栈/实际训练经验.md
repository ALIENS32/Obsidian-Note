# GRPO 

- 为什么 GRPO 很容易训飞，训到一半 reward 就很容易突然掉下来？
	-  知乎 https://www.zhihu.com/question/1893241692582285916/answer/1894340646103392797
	- 对于小模型而言，比如0.5B，1B大小的模型，以及部分7B和8B的模型，比如 [LLAMA](https://zhida.zhihu.com/search?content_id=722483501&content_type=Answer&match_order=1&q=LLAMA&zhida_source=entity)，这些模型的R1训练奖励是稀疏的。这时候在使用[GRPO](https://zhida.zhihu.com/search?content_id=722483501&content_type=Answer&match_order=1&q=GRPO&zhida_source=entity) 训练过程中，一次训练采样中有效的奖励值非常稀疏，因此训练很不稳定。
	- 稀疏和稠密是针对Reward的分布来说的，小模型本身能力差，那么回答正确的概率就低，相应的正反馈就少，可能一千次回答，一百次完成了目标，那么正向奖励只有1/10，所以说奖励是稀疏的。相反大模型能力强，做对的概率大，奖励就是相对更加稠密的。
	- 又 GRPO 是通过自身多个回答标准化出来的优势，但是小模型烂，正向回答很少，导致容易在跟自身的比较中学歪
	- 差生看自己笔记怎么学的好
