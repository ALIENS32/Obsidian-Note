#RL 
- **状态价值**
	- **定义**：状态值 $v_\pi(s)$ 是智能体（代理）从状态 $s$ 出发，遵循策略 $\pi$ 时，获得的折扣回报的期望值，可以用于评估策略。
	- **重要性**：用于评估策略，能产生更大状态值的策略相对更优 ，可帮助筛选、优化策略。
	- **计算**：借助贝尔曼方程来计算状态值，体现了状态值的递归求解逻辑，把当前状态价值和后续状态价值、即时奖励等关联起来 。
- **贝尔曼方程（Bellman Equation）** 
	- **推导**：贝尔曼方程*描述了所有状态值之间的关系*。通过将折扣回报$G_t$表示为当前奖励和未来回报的函数，推导出状态值的表达式。
	- **转移表达式**：$$
	
	\begin{align*}
	
	v_\pi(s) &= \mathbb{E}_\pi\left[ G_t \mid S_t = s \right] \\
	
	&= \mathbb{E}_\pi\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
	
	&= \sum_{a \in A} \pi(a \mid s) \left( \sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) \mathbb{E}_\pi\left[ G_{t+1} \mid S_{t+1} = s' \right] \right) \\
	
	&= \sum_{a \in A} \pi(a \mid s) \left( \sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) v_\pi(s') \right)
	
	\end{align*}
	
	$$
		最终简洁表达式：$$
	
	v_\pi(s) = \sum_{a \in A} \pi(a \mid s) \left( \sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) v_\pi(s') \right)
	
	$$
		（公式中，$\pi(a \mid s)$ 是策略 $\pi$ 下状态 $s$ 选动作 $a$ 的概率，$p(r \mid s, a)$ 是状态 $s$ 选动作 $a$ 获得奖励 $r$ 的概率，$p(s' \mid s, a)$ 是状态 $s$ 选动作 $a$ 转移到状态 $s'$ 的概率，$\gamma$ 是折扣因子 ）
	- **矩阵表达式**：
		- **定义**：把贝尔曼方程转化为矩阵向量形式，方便计算与分析。
		- **表达式**：$v_{\pi} = r_{\pi} + \gamma P_{\pi}v_{\pi}$ ，其中$v_{\pi}$ 通常是价值函数向量，$r_{\pi}$ 是奖励向量，$\gamma$ 是折扣因子（属于强化学习等领域常用参数 ），$P_{\pi}$ 是状态转移概率矩阵。
		- **性质**：
			- $I - \gamma P_{\pi}$ 可逆（$I$ 为单位矩阵 ）；
			- $(I - \gamma P_{\pi})^{-1} \geq I$ ；
			- 对任意 $r \geq 0$ ，有 $(I - \gamma P_{\pi})^{-1}r \geq r \geq 0$ ，这些性质在强化学习等理论推导、算法分析（如策略迭代、价值迭代 ）里用于保障解的存在性、单调性等，辅助求解最优策略与价值函数 。
- **动作价值**
	- **动作值定义**：动作值 $q_{\pi}(s, a)$ 指智能体（代理）在状态 $s$ 下执行动作 $a$ 后，获得折扣回报的期望值，用于衡量该状态 - 动作对的价值 。
	- **状态值与动作值关系**：
		- **状态值是动作值的期望**：$v_{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q_{\pi}(s, a)$ ，即状态 $s$ 的价值 $v_{\pi}(s)$ ，是基于策略 $\pi$（$\pi(a \mid s)$ 为状态 $s$ 下选动作 $a$ 的概率 ），对所有可能动作 $a$ 对应的动作值 $q_{\pi}(s, a)$ 求期望 。
		- **动作价值函数表达式**：$q_{\pi}(s, a)=\sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) v_{\pi}(s')$ ，其中 $p(r \mid s, a)$ 是状态 $s$ 选动作 $a$ 获得奖励 $r$ 的概率，$p(s' \mid s, a)$ 是状态 $s$ 选动作 $a$ 转移到状态 $s'$ 的概率，$\gamma$ 为折扣因子，体现未来回报的权重，该式表明动作值由即时奖励期望和后续状态价值期望（经折扣 ）组成。