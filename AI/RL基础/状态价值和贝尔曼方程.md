- 状态价值
	- **定义**：状态值$v_\pi(s)$是智能体（代理）从状态$s$出发，遵循策略$\pi$时，获得的折扣回报的期望值 。
	- **重要性**：用于评估策略，能产生更大状态值的策略相对更优 ，可帮助筛选、优化策略。
	- **计算**：借助贝尔曼方程来计算状态值，体现了状态值的递归求解逻辑，把当前状态价值和后续状态价值、即时奖励等关联起来 。
- **贝尔曼方程（Bellman Equation）** 
	- **推导**：贝尔曼方程描述了所有状态值之间的关系。通过将折扣回报$G_t$表示为当前奖励和未来回报的函数，推导出状态值的表达式。
	- **表达式**：$$
	
	\begin{align*}
	
	v_\pi(s) &= \mathbb{E}_\pi\left[ G_t \mid S_t = s \right] \\
	
	&= \mathbb{E}_\pi\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
	
	&= \sum_{a \in A} \pi(a \mid s) \left( \sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) \mathbb{E}_\pi\left[ G_{t+1} \mid S_{t+1} = s' \right] \right) \\
	
	&= \sum_{a \in A} \pi(a \mid s) \left( \sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) v_\pi(s') \right)
	
	\end{align*}
	
	$$
		最终简洁表达式：
	$$
	
	v_\pi(s) = \sum_{a \in A} \pi(a \mid s) \left( \sum_{r \in R} p(r \mid s, a) r + \gamma \sum_{s' \in S} p(s' \mid s, a) v_\pi(s') \right)
	
	$$
		（公式中，$\pi(a \mid s)$ 是策略 $\pi$ 下状态 $s$ 选动作 $a$ 的概率，$p(r \mid s, a)$ 是状态 $s$ 选动作 $a$ 获得奖励 $r$ 的概率，$p(s' \mid s, a)$ 是状态 $s$ 选动作 $a$ 转移到状态 $s'$ 的概率，$\gamma$ 是折扣因子 ）