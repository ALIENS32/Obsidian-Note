- 状态/状态空间：代理在环境中的位置或状态
- 动作/动作空间：代理在每个状态下可采取的行为
- 状态转移：代理采取动作后从一个状态转移到另一个状态
- 策略：agent在某个状态下应该采取哪些动作以及相应概率
	- 确定性策略
	- 随机性策略
- 奖励：agent执行动作后从环境获得的的反馈，依赖于当前状态和动作
- 轨迹：状态-动作-奖励的链
- 回报：轨迹上收集的所有奖励的总和，用于评估策略的好坏
- 折扣回报：对于无限长的轨迹，引入折扣率 $\gamma$ 来计算折扣回报，避免回报值发散
- MDP：
	- **MDP 的组成**： - 状态空间 \( S \)、动作空间 \( A(s) \)、奖励集合 \( R(s,a) \) 。 - 状态转移概率 \( p(s'|s,a) \) 和奖励概率 \( p(r|s,a) \) 。 - 策略 \( \pi(a|s) \) 。 - 马尔可夫性质：下一状态或奖励仅依赖于当前状态和动作，与之前的状态和动作无关 。 - **模型的稳定性**：考虑的是 stationary model（平稳模型），即模型不随时间变化 。 马尔可夫决策过程是强化学习等领域的重要理论基础，用于描述智能体在环境中进行决策与学习的过程 。
这段内容是关于马尔可夫决策过程（Markov Decision Process，简称 MDP ）的知识要点，识别如下：

- **MDP 的组成**：

- 状态空间 \( S \)、动作空间 \( A(s) \)、奖励集合 \( R(s,a) \) 。

- 状态转移概率 \( p(s'|s,a) \) 和奖励概率 \( p(r|s,a) \) 。

- 策略 \( \pi(a|s) \) 。

- 马尔可夫性质：下一状态或奖励仅依赖于当前状态和动作，与之前的状态和动作无关 。

- **模型的稳定性**：考虑的是 stationary model（平稳模型），即模型不随时间变化 。

马尔可夫决策过程是强化学习等领域的重要理论基础，用于描述智能体在环境中进行决策与学习的过程 。