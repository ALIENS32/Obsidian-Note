- 状态/状态空间：代理在环境中的位置或状态
- 动作/动作空间：代理在每个状态下可采取的行为
- 状态转移：代理采取动作后从一个状态转移到另一个状态
- 策略：agent在某个状态下应该采取哪些动作以及相应概率
	- 确定性策略
	- 随机性策略
- 奖励：agent执行动作后从环境获得的的反馈，依赖于当前状态和动作
- 轨迹：状态-动作-奖励的链
- 回报：轨迹上收集的所有奖励的总和，用于评估策略的好坏
- 折扣回报：对于无限长的轨迹，引入折扣率 $\gamma$ 来计算折扣回报，避免回报值发散
- MDP：




- **状态/状态空间**：智能体在环境中的位置或状态；所有可能状态的集合
- **动作/动作空间**：智能体在某状态下可执行的行为；该状态下所有可能行为的集合
- **状态转移**：智能体执行动作后，从当前状态切换到下一状态的过程
- **策略**：智能体在特定状态下选择动作的规则（含概率分布）
	- 确定性策略：明确指定某状态下的唯一动作
	- 随机性策略：给定某状态下每个动作的选择概率
- **奖励**：环境对智能体“状态-动作”组合的即时反馈（数值形式）
- **轨迹**：按时间顺序排列的状态-动作-奖励序列（如 $s_0,a_0,r_1,s_1,a_1,r_2,...$）
- **回报**：轨迹中所有奖励的累加和，用于衡量策略的长期效果
- **折扣回报**：对长期奖励引入折扣率 $\gamma$（$0<\gamma\le1$），计算累加值（如 $G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+...$），解决无限轨迹的回报发散问题
- **MDP（马尔可夫决策过程）**：含状态、动作、转移概率、奖励的模型，满足“下一状态仅依赖当前状态和动作”的马尔可夫性，是强化学习的核心框架