#RL 
- **状态/状态空间**：代理在环境中的位置或状态
- **动作/动作空间**：代理在每个状态下可采取的行为
- **状态转移**：代理采取动作后从一个状态转移到另一个状态
- **策略**：agent在某个状态下应该采取哪些动作以及相应概率
	- 确定性策略
	- 随机性策略
- **奖励**：agent执行动作后从环境获得的的反馈，依赖于当前状态和动作
- **轨迹**：状态-动作-奖励的链
- **回报**：轨迹上收集的所有奖励的总和，用于评估策略的好坏
- **折扣回报**：对于无限长的轨迹，引入折扣率$\gamma$来计算折扣回报，避免回报值发散
- **MDP 马尔可夫决策过程**：
	- **MDP 的组成**：
		- 状态空间$S$、动作空间$A(s)$、奖励集合$R(s,a)$。
		- 状态转移概率$p(s'|s,a)$和奖励概率$p(r|s,a)$。
		- 策略$\pi(a|s)$。
		- 马尔可夫性质：下一状态或奖励仅依赖于当前状态和动作，与之前的状态和动作无关 。
	- **模型的稳定性**：考虑的是 stationary model（平稳模型），即模型不随时间变化 。