- 强化学习的终极目标是**寻找最优策略**
- 改进策略的例子：
	- 书中给出了一个例子，即在网格世界中寻找奖励最大的路线，我们通过动作价值来评估策略，根据动作价值=动作奖励+转移到的状态的价值，所以第一步要根据当前策略算出所有状态的价值，然后再加上动作价值得到所有动作价值，那么对于当前所处的状态，选择所有动作中价值最大的就是最优策略
	- 核心，先算状态价值，再算动作价值，然后最大化动作价值来改进策略
- **最优策略与最优状态值**：策略$\pi^*$为最优策略的条件，即对于任意状态$s \in \mathcal{S}$（$\mathcal{S}$为状态集合 ）和任意其他策略$\pi$，都有$v_{\pi^*}(s) \geq v_{\pi}(s)$（$v_{\pi}(s)$表示策略$\pi$在状态$s$下的状态值 ）。
---------
- 问题：
	1. **存在性**：探讨是否存在满足定义的最优策略 。
	2. **唯一性**：探究符合条件的最优策略是仅有一个，还是可能有多个 。
	3. **随机性**：考察最优策略的性质，是确定性（给定状态，策略确定选某一行动）还是随机性（给定状态，策略以概率分布选行动 ）。
	4. **算法**：询问用于求解最优策略和对应最优状态值的具体方法，像强化学习里的价值迭代、策略迭代等算法常涉及 。
- 贝尔曼最优方程：$$

\begin{align*}

v(s)&=\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)\left(\sum_{r\in\mathcal{R}}p(r|s,a)r + \gamma\sum_{s'\in\mathcal{S}}p(s'|s,a)v(s')\right)\\

&=\max_{\pi(s)\in\Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)q(s,a)

\end{align*}

$$